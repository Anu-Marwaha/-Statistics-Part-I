{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Anu-Marwaha/-Statistics-Part-I/blob/master/CLASS_120_NAIVE_BAYES.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Um_VPiDU3YPj"
      },
      "source": [
        "# Naive Bayes\n",
        "\n",
        "Naive Bayes algorithm is a supervised machine learning algorithm based on the Bayes Probability theorem. Naive Bayes assumes that there is no correlation between the features in a dataset used to train the model. We will get back to this later.\n",
        "\n",
        "Despite the oversimplified assumptions, Naive Bayes works very well in many real world complex problems. They require a relatively small number of training data samples to perform classification efficiently, compared to other algorithms like Logistic Regression and Decision trees, that we studied earlier.\n",
        "\n",
        "\\\n",
        "# Bayes Theorem\n",
        "\n",
        "Bayes theorem describes the probability of a feature, based on prior knowledge of situations related to that feature.\n",
        "\n",
        "For example, if the probability someone having diabetes is related to his or her age, then by using the Bayes Theorem, the age can be used to more accurately predict the probability of having diabetes.\n",
        "\n",
        "\\\n",
        "#Naive\n",
        "\n",
        "The word **naive** implies that every pair of features in the dataset is independent of each other. Naive Bayes works on the assumption that the value of a particular feature is independent of any other feature. \n",
        "\n",
        "For example, A vegetable may be classified as a tomato if it's round, about 4 cm in diameter, and red in color. With Naive Bayes, each of these three features (shape, size and color) contributes independently to the probability that the vegetable is a tomato. Also, it assumes that there is no possible correlation between the shape, size and color.\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "\\\n",
        "It's absolutely fine if you're not holding up with all this information. Many people believe that Logistic Regression and Naive Bayes are similar in nature that give similar outcomes, and thus, get's confused on when to use what.\n",
        "\n",
        "\\\n",
        "Let's write some code to understand the differences between the two as we try to understand Naive Bayes a little more."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7QOZeXQMBkeW",
        "outputId": "944daa0a-3d06-41e1-ec20-5636c19353ab",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 248
        }
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv('https://raw.githubusercontent.com/whitehatjr/datasets/master/C120/diabetes.csv')\n",
        "\n",
        "print(df.head())\n",
        "df.corr()"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   glucose  bloodpressure  diabetes\n",
            "0       40             85         0\n",
            "1       40             92         0\n",
            "2       45             63         1\n",
            "3       45             80         0\n",
            "4       40             73         1\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                glucose  bloodpressure  diabetes\n",
              "glucose        1.000000      -0.164553  0.031585\n",
              "bloodpressure -0.164553       1.000000 -0.808303\n",
              "diabetes       0.031585      -0.808303  1.000000"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-dab2af89-d810-4415-8581-569e2f15cb90\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>glucose</th>\n",
              "      <th>bloodpressure</th>\n",
              "      <th>diabetes</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>glucose</th>\n",
              "      <td>1.000000</td>\n",
              "      <td>-0.164553</td>\n",
              "      <td>0.031585</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>bloodpressure</th>\n",
              "      <td>-0.164553</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>-0.808303</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>diabetes</th>\n",
              "      <td>0.031585</td>\n",
              "      <td>-0.808303</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-dab2af89-d810-4415-8581-569e2f15cb90')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-dab2af89-d810-4415-8581-569e2f15cb90 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-dab2af89-d810-4415-8581-569e2f15cb90');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6vecpIyjB_Va"
      },
      "source": [
        "In the data that we have, we can see that we have **glucose**, **bloodpressure** and we know if the given person has **diabetes** or not.\n",
        "\n",
        "Here, we will use the **glucose** and the **bloodpressure** to predict if the person has diabetes or not using Naive Bayes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vkI-AupNB-Ju"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X = df[[\"glucose\", \"bloodpressure\"]]\n",
        "y = df[\"diabetes\"]\n",
        "\n",
        "x_train_1, x_test_1, y_train_1, y_test_1 = train_test_split(X, y, test_size=0.25, random_state=42)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zE3SBeQ3DPiS"
      },
      "source": [
        "## Training the model with naive Bayes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7D8JrbUoDMX-",
        "outputId": "67a56618-7a51-49a3-fc3c-46afdd5eb92f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.preprocessing import StandardScaler \n",
        "\n",
        "sc = StandardScaler()\n",
        "\n",
        "x_train_1 = sc.fit_transform(x_train_1) \n",
        "x_test_1 = sc.fit_transform(x_test_1) \n",
        "\n",
        "model_1 = GaussianNB()\n",
        "model_1.fit(x_train_1, y_train_1)\n",
        "\n",
        "y_pred_1 = model_1.predict(x_test_1)\n",
        "\n",
        "accuracy = accuracy_score(y_test_1, y_pred_1)\n",
        "print(accuracy)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.9437751004016064\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NFTRRr9yDw0U"
      },
      "source": [
        "Here, we can see that we have an accuracy of approximately an outstanding 94.4%. Let's see if using logistics regression would have given us the same accuracy?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6e5OY4iSDoFd"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X = df[[\"glucose\", \"bloodpressure\"]]\n",
        "y = df[\"diabetes\"]\n",
        "\n",
        "x_train_2, x_test_2, y_train_2, y_test_2 = train_test_split(X, y, test_size=0.25, random_state=42)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bvw4yBfBLksO"
      },
      "source": [
        "## Training the model with Logistic Regression"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PZdXwR7nFTr1",
        "outputId": "97030713-7c90-4249-af00-e635cf49d6c5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression \n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.preprocessing import StandardScaler \n",
        "\n",
        "sc = StandardScaler()\n",
        "\n",
        "x_train_2 = sc.fit_transform(x_train_2) \n",
        "x_test_2 = sc.fit_transform(x_test_2) \n",
        "\n",
        "model_2 = LogisticRegression(random_state = 0) \n",
        "model_2.fit(x_train_2, y_train_2)\n",
        "\n",
        "y_pred_2 = model_2.predict(x_test_2)\n",
        "\n",
        "accuracy = accuracy_score(y_test_2, y_pred_2)\n",
        "print(accuracy)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.9156626506024096\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nsk6ZDqiFt-q"
      },
      "source": [
        "While the accuracy score for both the datasets was close, with Naive Bayes giving us an accuracy of **94.4%** and logistic regression giving us an accuracy of **91.6%**, Naive Bayes still performed better.\n",
        "\n",
        "The reason for this is that if we look at our features again, we can see that the Glucose and the Blood Pressure had no correlation with each other. They both contributed individually to whether a person would have diabetes or not. This is exactly what Naive Bayes algorithm assumes, that all the features contribute individually to the outcome.\n",
        "\n",
        "\\\n",
        "This was for the case of where Naive Bayes outperforms Logistic Regression, but let's see an example of the case where Logistic Regression outperforms Naive Bayes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p5dZWtSG8BvW"
      },
      "source": [
        "#Studnet side code "
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zo8VwKkTFsx8"
      },
      "source": [],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y2afzLZAIvTk",
        "outputId": "2c0f2f57-8fc6-4082-abf9-3bb06343aaa7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv('https://raw.githubusercontent.com/whitehatjr/datasets/master/C120/income.csv')\n",
        "\n",
        "print(df.head())\n",
        "print(df.describe())"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   age          workclass education_level  education-num       marital-status  \\\n",
            "0   39          State-gov       Bachelors           13.0        Never-married   \n",
            "1   50   Self-emp-not-inc       Bachelors           13.0   Married-civ-spouse   \n",
            "2   38            Private         HS-grad            9.0             Divorced   \n",
            "3   53            Private            11th            7.0   Married-civ-spouse   \n",
            "4   28            Private       Bachelors           13.0   Married-civ-spouse   \n",
            "\n",
            "           occupation    relationship    race      sex  capital-gain  \\\n",
            "0        Adm-clerical   Not-in-family   White     Male        2174.0   \n",
            "1     Exec-managerial         Husband   White     Male           0.0   \n",
            "2   Handlers-cleaners   Not-in-family   White     Male           0.0   \n",
            "3   Handlers-cleaners         Husband   Black     Male           0.0   \n",
            "4      Prof-specialty            Wife   Black   Female           0.0   \n",
            "\n",
            "   capital-loss  hours-per-week  native-country income  \n",
            "0           0.0            40.0   United-States  <=50K  \n",
            "1           0.0            13.0   United-States  <=50K  \n",
            "2           0.0            40.0   United-States  <=50K  \n",
            "3           0.0            40.0   United-States  <=50K  \n",
            "4           0.0            40.0            Cuba  <=50K  \n",
            "                age  education-num  capital-gain  capital-loss  hours-per-week\n",
            "count  45222.000000   45222.000000  45222.000000  45222.000000    45222.000000\n",
            "mean      38.547941      10.118460   1101.430344     88.595418       40.938017\n",
            "std       13.217870       2.552881   7506.430084    404.956092       12.007508\n",
            "min       17.000000       1.000000      0.000000      0.000000        1.000000\n",
            "25%       28.000000       9.000000      0.000000      0.000000       40.000000\n",
            "50%       37.000000      10.000000      0.000000      0.000000       40.000000\n",
            "75%       47.000000      13.000000      0.000000      0.000000       45.000000\n",
            "max       90.000000      16.000000  99999.000000   4356.000000       99.000000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x_zI9gQlKMZU"
      },
      "source": [
        "From the given data, we will consider the following fields to determine the salary of a person -\n",
        "\n",
        "\n",
        "\n",
        "1.   Age\n",
        "2.   Hours Per Week\n",
        "3.   Education Number\n",
        "4.   Capital Gain\n",
        "5.   Capital Loss"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tHP4vXJ8JsSP"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X = df[[\"age\", \"hours-per-week\", \"education-num\", \"capital-gain\", \"capital-loss\"]]\n",
        "y = df[\"income\"]\n",
        "\n",
        "x_train_1, x_test_1, y_train_1, y_test_1 = train_test_split(X, y, test_size=0.25, random_state=42)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ommNAshKLqWz"
      },
      "source": [
        "## Training the model with Naive Bayes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GQqZZStNLhGi",
        "outputId": "3d66aa26-d911-44b0-a640-bdc95a920ccc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.preprocessing import StandardScaler \n",
        "\n",
        "sc = StandardScaler()\n",
        "\n",
        "x_train_1 = sc.fit_transform(x_train_1) \n",
        "x_test_1 = sc.fit_transform(x_test_1) \n",
        "\n",
        "model_1 = GaussianNB()\n",
        "model_1.fit(x_train_1, y_train_1)\n",
        "\n",
        "y_pred_1 = model_1.predict(x_test_1)\n",
        "\n",
        "accuracy = accuracy_score(y_test_1, y_pred_1)\n",
        "print(accuracy)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.7896692021935255\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PPURq8TwMhSp"
      },
      "source": [
        "This time, with the new dataset, we can see that Naive Bayes gave us an accuracy of almost **79%**. Let's see how much accuracy do we get with Logistic Regression."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YAs37hE_Ludo"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X = df[[\"age\", \"hours-per-week\", \"education-num\", \"capital-gain\", \"capital-loss\"]]\n",
        "y = df[\"income\"]\n",
        "\n",
        "x_train_2, x_test_2, y_train_2, y_test_2 = train_test_split(X, y, test_size=0.25, random_state=42)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EGXuObEUNXpH"
      },
      "source": [
        "## Training model with Logistic Regression"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OQKN9r-rNO8G",
        "outputId": "97db1f25-857a-4302-93cb-2b74748f4605",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression \n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.preprocessing import StandardScaler \n",
        "\n",
        "sc = StandardScaler()\n",
        "\n",
        "x_train_2 = sc.fit_transform(x_train_2) \n",
        "x_test_2 = sc.fit_transform(x_test_2) \n",
        "\n",
        "model_2 = LogisticRegression(random_state = 0) \n",
        "model_2.fit(x_train_2, y_train_2)\n",
        "\n",
        "y_pred_2 = model_2.predict(x_test_2)\n",
        "\n",
        "accuracy = accuracy_score(y_test_2, y_pred_2)\n",
        "print(accuracy)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.8116929064213692\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vBUftfATNcc8"
      },
      "source": [
        "With Logistic Regression, this time, we got an accuracy of **81.1%**. Let's study this more closely.\n",
        "\n",
        "\\\n",
        "# Difference b/w Naive Bayes and Logistic Regression\n",
        "\n",
        "\\\n",
        "In the first dataset, as we pointed out earlier, both the *glucose* and the *bloodpressure* had little correlation, and both of them were contributing individually to whether a person has diabetes or not.\n",
        "\n",
        "\\\n",
        "**Conclusion**\n",
        "In these kinds of dataset, where all the features contribute individually to the outcome, Naive Bayes outperforms logistic regression and is highly efficient.\n",
        "\n",
        "\\\n",
        "In the second dataset, Logistic Regression outperformed Naive Bayes. The reason is that in this dataset, not all features contribute individually to the outcome. For example, there have been people of all age groups earning both less than and more than 50K. There have also been people with all education numbers that have an income of both less and more than 50K. Here, the combination of all the features is a better predictor of whether a person is earning more than or less than 50K, instead of all features having their individual contribution."
      ]
    }
  ]
}